# This config file contains the default settings for all orthoseg projects.
#
# The config used for an orthoseg project is loaded in the following order:
#   1) the project defaults as "hardcoded" in orthoseg (project_defaults.ini)
#   2) any .ini files specified in the general.extra_config_files_to_load 
#      parameter (in this file).
#   3) the project config file
# Parameters specified in a config file loaded later in the order above
# overrule the corresponding parameter values specified in a previously 
# loaded config file.

# General settings.
[general]
# Extra config files to load for the project. They will be loaded in the 
# order specified and can be specified one path per line, comma seperated.
# If a relative path is used it will be resolved towards the parent dir of 
# the project config file.
extra_config_files_to_load =

# The subject that will be segmented -> must be overruled in the project 
# specific config file!!!
segment_subject = MUST_OVERRIDE

# Set the way certificates of ssl requests are verified:
#   * True: use the default certificate bundle as installed on your system
#   * False: disable certificate validation (NOT recommended!)
#   * path to a certificate bundle file (.pem) to specify the certificate 
#     bundle to be used. In corporate networks using a proxy server this is 
#     often needed when requests give CERTIFICATE_VERIFY_FAILED errors.
ssl_verify = True

# Specify the number of cpu's to use while doing heavy processing. If -1, it 
# uses all available processors.
nb_parallel = -1

# Settings regarding the download action
[download]
# Schedule to control when images can be downloaded. If not specified there is 
# no time limitation.  
cron_schedule

# Settings concerning the neural network model you want to use for the 
# segmentation. 
[model]
# The id of the architecture used. 
# Only needs to be changed if you want to compare the results of different  
# architectures on the same training data. 
# Reason: OrthoSeg will only train one model per traindata_id, architecture_id 
# and hyperparams_id.
# So if you want to compare different architectures, give each architecture a 
# unique id.
# Remark: if the architecture_id is 0, it won't be included in the file name 
# of trained models.
architecture_id = 0

# The segmentation architecture to use. 
# 
# The architectures currently supported by orthoseg follow the encoder-decoder 
# principle:
#   * an encoder: a (deep) neural network that detects features on object level
#   * a decoder: a (deep) neural network that converts the detected features 
#     on object level to a segmentation on pixel level
#
# To configure an encoder/decoder architecture in orthoseg, specify the it in 
# the following way: architecture = {encoder}+{decoder}
#
# There are a lot of encoders and decoders supported.
# For starters, the following encoder/decoder combinations are available:
#   * decoder: unet
#   * encoder: unet, ternaus 
# In the configuration, this can be specified as such, eg.:
#   * architecture = standard+unet
#
# Additionally, all encoders/decoders as provided by the 'segmentation_models'
# project (https://github.com/qubvel/segmentation_models#models-and-backbones) 
# can be used. 
# In the configuration below, use the "backbone" as encoder, and the "model" 
# as decoder, eg.:
#   * architecture = inceptionresnetv2+unet
architecture = inceptionresnetv2+unet

# The number of channels of the images to train on
nb_channels = 3

# Settings concerning the train process.
[train]
# Preload model -> only overrule in local_overrule.ini!
preload_with_previous_traindata = False
# Force to use a model trained on this traindata version (-1 to disable)
force_model_traindata_id = -1
# When training, resume training on the current best existing model
resume_train = False
# Train a model, even if a model exists already
force_train = False

# Pattern how the file paths of the label files should be formatted.
# The image_layer to get the images from is extracted from the file path. 
labelpolygons_pattern = ${dirs:labels_dir}/${general:segment_subject}_{image_layer}_polygons.gpkg
labellocations_pattern = ${dirs:labels_dir}/${general:segment_subject}_{image_layer}_locations.gpkg

# Column where the labels for each training polygon is available. If the column
# name configured here is not available columns "label_name" is tried as well. 
labelname_column = classname

# Parameters regarding the size/resolution of the images used to train on.
#
# The size the label_location boxes need to be digitized depends on these values:
# e.g. with image_pixel_width = 512 and image_pixel_x_size = 0.25, the boxes need to be
# 512 pixels * 0.25 meter/pixel = 128 meter wide.
#
# For some model architectures there are limitations on the image sizes
# supported. E.g. if you use the linknet decoder, the images pixel width and height
# has to be divisible by factor 32.
image_pixel_width = 512
image_pixel_height = 512
image_pixel_x_size = 0.25
image_pixel_y_size = 0.25

# The id of the set of hyper parameters to use while training. 
# Only needs to be changed if you want to compare the results of different  
# hyperparameter sets on the same training data. 
# Reason: OrthoSeg will only train one model per traindata_id, architecture_id and hyperparams_id.
# So if you want to compare different hyperparameters, give each set a unique id.
# If the hyperparams_id is 0, it won't be included in the file name of trained models.
trainparams_id = 0

# Image augmentations in json format
image_augmentations = { "fill_mode": "constant",
                        "cval": 0,
                        "rescale": 0.0039215686274509803921568627451,
                        "rotation_range": 359.0,
                        "width_shift_range": 0.05,
                        "height_shift_range": 0.05,
                        "zoom_range": 0.1,
                        "brightness_range": [0.95, 1.05]
                    }

# Mask augmentations in json format. 
# Remarks: 
#   * the number of randomized values must be the same as for the image, 
#     otherwise the random augentation factors aren't the same as the image!
#   * augmentations to translate, rotate,... should be the same as for the image!
#   * the mask generally shouldn't be rescaled!
#   * cval values should always be 0 for the mask, even if it is different for 
#     the image, as the cval of the mask refers to these locations being 
#     of class "background".
mask_augmentations = { "fill_mode": "constant",
                        "cval": 0,
                        "rescale": 1,
                        "rotation_range": 359.0,
                        "width_shift_range": 0.05,
                        "height_shift_range": 0.05,
                        "zoom_range": 0.1,
                        "brightness_range": [1.0, 1.0]
                    }

# In json format, the classes to train/predict and for each class:
#     * the label names in the training data to use for this class  
#     * the weight to use when training
classes =   {   "background": {
                    "labelnames": ["ignore_for_train", "background"],
                    "weight": 1
                },
                "${general:segment_subject}": {
                    "labelnames": ["${general:segment_subject}"],
                    "weight": 1
                }
            }

# The batch size to use during fit of model. 
# Depends on available hardware, model used and image size.
batch_size_fit = 6
# The batch size to use while predicting in the train process. 
# Depends on available hardware, model used and image size.
batch_size_predict = 20

# Optimizer to use + params
optimizer = adam
optimizer_params = { "learning_rate": 0.0001 } 

# Loss function to use: if not specified: automatic: 
#   - if weights specified in the classes: weighted_categorical_crossentropy
#   - if no weights are specified: categorical_crossentropy
loss_function

# The metric(s) to monitor to evaluate which network is best during the 
# training. This can be a single metric or a formula with placeholders 
# to calculate a value based on multiple metrics. Available metrics:
#   - one_hot_mean_iou: intersection over union on the training dataset
#   - val_one_hot_mean_iou: intersection over union on the validation dataset
#   - categorical_accuracy: accuracy on the training dataset
#   - val_categorical_accuracy: accuracy on the validation dataset
monitor_metric = ({one_hot_mean_iou}+{val_one_hot_mean_iou})/2
# Use max to keep the models with a high monitor_metric. Otherwise use min.
monitor_metric_mode = max

# Format to save the trained model in. Options are h5 or tf.
save_format = h5
# True to only keep the best model during training
save_best_only = True
# Minimum accuracy to save, (0 = always save)
save_min_accuracy = 0.80

# Number of epochs to train with some frozen layers. Keeps pretrained layers 
# intact, which is especially usefull for the first few (2-10) epochs when 
# big adjustments are made to the network. Training is also 20% faster during 
# these epochs.   
nb_epoch_with_freeze = 20
# Maximum number of epochs to train (without frozen layers). 
# These epochs are in addition to nb_epoch_with_freeze.
max_epoch = 1000

# Stop training if earlystop_monitor_metric hasn't improved for 
# earlystop_patience epochs
earlystop_patience = 100
# Options for the earlystop metric: categorical_accuracy, one_hot_mean_iou
earlystop_monitor_metric = one_hot_mean_iou
# Use max if the monitor_metric should be high. Otherwise use min.
earlystop_monitor_metric_mode = max

# True to activate tensorboard style logging 
log_tensorboard = False
# True to activate csv logging 
log_csv = True

# Subdir name to save augmented images to while training (only for debugging)
save_augmented_subdir

# Settings concerning the prediction process.
[predict]
# The batch size to use. 
# Depends on available hardware, model used and image size.
batch_size = 4

# To be able to predict, image_layer must be an existing layer in imagelayers.ini.
image_layer = MUST_OVERRIDE

# Parameters regarding the size/resolution of the images to run predict on.
#
# For some model architectures there are limitations on the image sizes
# supported. E.g. if you use the linknet decoder, the images pixel width and height
# has to be divisible by factor 32.
image_pixel_width = 2048
image_pixel_height = 2048
image_pixel_x_size = 0.25
image_pixel_y_size = 0.25
image_pixels_overlap = 128

# The minimum probability that a pixel needs to obtain for a class not to be treated as
# background.
min_probability = 0.5

# Maximum errors that can occur during prediction before prediction process is stopped.
max_prediction_errors = 100

# Config for the cleanup operations that are done on the predictions on-the-fly.

# Apply a filter to the background pixels and replace background by the most occuring
# value in a rectangle around the background pixel of the size specified.
filter_background_modal_size = 0

# Reclassify all detected polygons that comply to the query provided to the class of
# the neighbour with the longest border with it.
# The query need to be in the form to be used by pandas.DataFrame.query() and the
# following columns are available to query on:
#   - area: the area of the polygon, as calculated by GeoSeries.area .
#   - perimeter: the perimeter of the polygon, as calculated by GeoSeries.length .
#   - onborder: 1 if the polygon touches the border of the tile being predicted, 0 if
#     it doesn't. It is often useful to filter with onborder == 0 to avoid eg. polygons
#     being reclassified because they are small due to being on the border.
# Eg.: reclassify_to_neighbour_query = (onborder == 0 and area <= 5)
reclassify_to_neighbour_query

# Algorithm to use if vector simplification needs to be executed:
#   * RAMER_DOUGLAS_PEUCKER: 
#       * simplify_tolerance: extra, mandatory parameter: specifies the distance 
#         tolerance to be used.
#   * VISVALINGAM_WHYATT: 
#       * simplify_tolerance: extra, mandatory parameter: specifies the area 
#         tolerance to be used.
#   * LANG: gives few deformations and removes many points
#       * simplify_tolerance: extra, mandatory parameter: specifies the distance 
#         tolerance to be used.
#       * simplify_lookahead: extra, mandatory parameter: specifies the number  
#         of points the algorithm looks ahead during simplify.
#   * LANG+: gives few deformations while removeing the most points.
#       * simplify_tolerance: extra, mandatory parameter: specifies the distance 
#         tolerance to be used.
#       * simplify_lookahead: extra, mandatory parameter: specifies the number  
#         of points the algorithm looks ahead during simplify.
#   * If simplify_algorithm is not specified, no simplification is applied.  
simplify_algorithm = LANG+

# Tolerance to use for the simplification. 
# Remark: you can use simple math expressions, eg. 1.5*5 
simplify_tolerance = ${image_pixel_x_size}*1.5
# For algorythms that need this, specifies the number of points to look ahead
# during simplify. Used for LANG.
simplify_lookahead = 8
# If True, the resulting polygons of the classification are converted to topologies so
# no gaps are introduced in polygons that are next to each other. If not specified
# (= None), a multi-class classification will be simplified topologically, a single
# class will be simplified the standard way.
simplify_topological

# Settings concerning the postprocessing after the prediction.
[postprocess]

# If dissolve is true, the result is dissolved.
dissolve = True
# If a path is provided, the result of the dissolve will be tiled on the tiles 
# provided.
dissolve_tiles_path

# Reclassify all detected polygons that comply to the query provided to the class of
# the neighbour with the longest border with it.
# The query need to be in the form to be used by pandas.DataFrame.query() and the
# following columns are available to query on:
#   - area: the area of the polygon, as calculated by GeoSeries.area.
#   - perimeter: the perimeter of the polygon, as calculated by GeoSeries.length.
# Eg.: reclassify_to_neighbour_query = (area <= 5)
reclassify_to_neighbour_query

# Apply simplify (also) after dissolve. For more information, check out the 
# documentation at parameter predict:simplify_algorithm
simplify_algorithm

# Tolerance to use for the postprocess simplification. 
# Remark: you can use simple math expressions, eg. 1.5*5 
simplify_tolerance = ${predict:image_pixel_x_size}*2

# For algorythms that need this, specifies the number of points to look ahead
# during simplify. Used for LANG.
simplify_lookahead = 8

# Settings concerning the directories where input/output data is found/put.
[dirs]
# Remarks: 
#   * UNC paths are not supported on Windows, always use mapped drive letters!
#   * always use forward slashes, even on Windows systems
#   * in all paths, it is possible to use the {tempdir} placeholder, which will 
#     be replaced by the default system temp dir. 

# The base projects dir, where multiple orthoseg projects can be stored. Can either be 
#   * an absolute path 
#   * OR a relative path starting from the location of the specific projectconfig file of the project
# Eg.: ".." means: projects_dir is the parent dir of the dir containing the project config file
projects_dir = ..

# The project directory for this subject
project_dir = ${projects_dir}/${general:segment_subject}

# Log dir
log_dir = ${project_dir}/log

# Dir containing the label data
labels_dir = ${project_dir}/labels

# Dirs used to put data during training 
training_dir = ${project_dir}/training

# Model dir
model_dir = ${project_dir}/models

# Output vector dir
output_vector_dir = ${project_dir}/output_vector/${predict:image_layer}

# Dir with the images we want predictions for
base_image_dir = ${projects_dir}/_image_cache
predict_image_input_subdir = ${predict:image_pixel_width}x${predict:image_pixel_height}_${predict:image_pixels_overlap}pxOverlap
predict_image_input_dir = ${base_image_dir}/${predict:image_layer}/${predict_image_input_subdir}
predict_image_output_basedir = ${predict_image_input_dir}

# Dir with sample images for use during training
# Remark: these samples are meant to check the training quality, so by default
#         the train image size is used!!! 
predictsample_image_input_subdir = ${train:image_pixel_width}x${train:image_pixel_height}
predictsample_image_input_dir = ${base_image_dir}/${predict:image_layer}_testsample/${predictsample_image_input_subdir}
predictsample_image_output_basedir = ${predictsample_image_input_dir}

# Settings concerning some specific file paths.
[files]
# File path that will be used to save/load the keras model definition
model_json_filepath = ${dirs:model_dir}/${model:architecture}.json
image_layers_config_filepath = ${dirs:projects_dir}/imagelayers.ini

# File path of file that if it exists cancels the current processing
cancel_filepath = ${dirs:projects_dir}/cancel.txt

# Email config to use to send progress info to. 
[email]
# Set enabled to True to enable sending mails
enabled = False
# Email address to send task status info from
from = sample@samplemail.be
# Email address to send task status info to
to = sample@samplemail.be
# Smtp server to use 
smtp_server = server.for.emails.be
# Username to use to login to smtp server (in some cases optional)
mail_server_username = 
# Password to use to login to smtp server (in some cases optional)
mail_server_password = 

# Logging configuration. 
[logging]
# The number of log files to keep in a log dir
nb_logfiles_tokeep = 10

# Config to use for the logging. This config is in json, following the 
# conventions as required by logging.dictConfig.
# https://docs.python.org/3/library/logging.config.html#logging-config-dictschema 
# 
# Mind: the location for file logging 
logconfig = {
        "version": 1,
        "disable_existing_loggers": true,
        "formatters": {
            "console": {
                "format": "%(asctime)s.%(msecs)03d|%(levelname)s|%(name)s|%(message)s", 
                "datefmt": "%H:%M:%S"
                },
            "file": {
                "format": "%(asctime)s|%(levelname)s|%(name)s|%(message)s", 
                "datefmt": null
            }
        },
        "handlers": {
            "console": {
                "level": "INFO",
                "class": "logging.StreamHandler",
                "formatter": "console",
                "stream": "ext://sys.stdout"
            },
            "file": {
                "level": "INFO",
                "class": "logging.handlers.RotatingFileHandler",
                "formatter": "file",
                "filename": "_log/{iso_datetime}.log",
                "maxBytes": 10000000,
                "backupCount": 3
            }
        },
        "loggers": {
            "geofile_ops": {
                "level": "INFO",
                "handlers": ["console"],
                "propagate": false
            },
            "geofile_ops.geofile_ops": {
                "level": "DEBUG",
                "handlers": ["console"],
                "propagate": false
            }        
        },
        "root": {
            "level": "INFO",
            "handlers": ["console", "file"]
        }
    }