# This is a config file with the default settings for all orthoseg projects.

[general]
# The subject that will be segmented -> must be overruled in the project specific config file!!!
segment_subject = MUST_OVERRIDE

[model]
# The id of the architecture used. 
# Only needs to be changed if you want to compare the results of different architectures on the same 
# training data. 
# Reason: OrthoSeg will only train one model per traindata_id, architecture_id and hyperparams_id.
# So if you want to compare different architectures, give each architecture a unique id.
# Remark: if the architecture_id is 0, it won't be included in the file name of trained models.
architecture_id = 0
# The segmentation architecture to use
encoder = inceptionresnetv2
decoder = unet
architecture = ${encoder}+${decoder}
# The number of channels of the images to train on
nb_channels = 3

[train]
# Preload model -> only overrule in local_overrule.ini!
preload_with_previous_traindata = False
# Force to use a model trained on this traindata version (-1 to disable)
force_model_traindata_id = -1
# When training, resume training on the current best existing model
resume_train = False
# Train a model, even if a model exists already 
force_train = False

# Image layer to use to get train images from
# Remark: only needs to be defined if only one label datasource is used. If 
# multiple label_datasources are defined, the need to be hardcoded there.
image_layer = MUST_OVERRIDE

# Info about the label files to be used, in json format.
# Remark: it is possible to define/use multiple label data sources
label_datasources = {   "label_ds0":{   
                            "locations_path": "${dirs:labels_dir}/${general:segment_subject}_labellocations_${image_layer}.gpkg",
                            "data_path": "${dirs:labels_dir}/${general:segment_subject}_labeldata_${image_layer}.gpkg",
                            "image_layer": "${train:image_layer}"
                        }
                    }

# Info about the images used to train on
image_pixel_width = 512
image_pixel_height = 512
image_pixel_x_size = 0.25
image_pixel_y_size = 0.25

# The id of the set of hyper parameters to use while training. 
# Only needs to be changed if you want to compare the results of different hyperparameter sets on 
# the same training data. 
# Reason: OrthoSeg will only train one model per traindata_id, architecture_id and hyperparams_id.
# So if you want to compare different hyperparameters, give each set a unique id.
# If the hyperparams_id is 0, it won't be included in the file name of trained models.
trainparams_id = 0

# Image augmentations in json format
# Removed "brightness_range": (0.95,1.05)
image_augmentations = { "fill_mode": "constant",
                        "cval": 0,
                        "rescale": 0.0039215686274509803921568627451,
                        "rotation_range": 359.0,
                        "width_shift_range": 0.05,
                        "height_shift_range": 0.05,
                        "zoom_range": 0.1
                    }

# Mask augmentations in json format
# Remark: the number of randomized values must be the same as for the image, 
# otherwise the random augentations of the mask aren't the same as the image!!!
# Removed "brightness_range": (0.95,1.05)
mask_augmentations = ${image_augmentations}

# In json format, label names to burn and for each class:
#     * the value to attribute to them in the mask
#     * the weight to use when training
classes =   {   "background": {
                    "burn_value": 0,
                    "weight": 1
                },
                "${general:segment_subject}": {
                    "burn_value": 255,
                    "weight": 5
                }
            }

# The batch size to use during fit of model. 
# Depends on available hardware, model used and image size.
batch_size_fit = 6
# The batch size to use while predicting in the train process. 
# Depends on available hardware, model used and image size.
batch_size_predict = 20

# Optimizer to use + params
optimizer = adam
optimizer_params = { "learning_rate": 0.0001 } 

# Loss function to use
loss_function = weighted_categorical_crossentropy

# The metric(s) to monitor to evaluate which network is best during the training.
# This can be a single metric or a formula with placeholders to calculate a value
# based on multiple metrics
monitor_metric = ({categorical_accuracy}+{f1-score})/2
# Whether the monitor_metric if best if a high value (max) of a low value (min)
monitor_metric_mode = max

# Format to save the trained model in
save_format = h5
# True to only keep the best model during training
save_best_only = True
# Minimum accuracy to save, (0 = always save)
save_min_accuracy = 0.80

# Maximum number of epochs to train
max_epoch = 1000
# Stop training if earlystop_monitor_metric hasn't improved for earlystop_patience epochs
earlystop_patience = 100
earlystop_monitor_metric = categorical_accuracy

[predict]
# The batch size to use. 
# Depends on available hardware, model used and image size.
batch_size = 4

# Info about the source images that need to be segmented.
image_layer = MUST_OVERRIDE
image_pixel_width = 2048
image_pixel_height = 2048
image_pixel_x_size = 0.25
image_pixel_y_size = 0.25
image_pixels_overlap = 128

[postprocess]
# If a path is provided, the result of the dissolve will be tiled on the tiles provided
dissolve_tiles_path

[dirs]
# Remarks: 
#     - UNC paths are not supported on Windows, always use mapped drive letters!
#     - always use forward slashes, even on Windows systems

# The base projects dir, where multiple orthoseg projects can be stored. Can either be 
#     * an absolute path 
#     * OR a relative path starting from the location of the specific projectconfig file of the project
# Eg.: ".." means: projects_dir is the parent dir of the dir containing the project config file
projects_dir = ..

# The project directory for this subject
project_dir = ${projects_dir}/${general:segment_subject}

# Training dirs in the project
labels_dir = ${project_dir}/labels
training_dir = ${project_dir}/training

# Log dir
log_dir = ${project_dir}/log

# Model dir
model_dir = ${project_dir}/models

# Output vector dir
output_vector_dir = ${project_dir}/output_vector

# Dir with the images we want predictions for
base_image_dir = ${projects_dir}/_image_cache
predict_image_input_subdir = ${predict:image_pixel_width}x${predict:image_pixel_height}_${predict:image_pixels_overlap}pxOverlap
predict_image_input_dir = ${base_image_dir}/${predict:image_layer}/${predict_image_input_subdir}
predict_image_output_basedir = ${predict_image_input_dir}

# Dir with sample images for use during training
# Remark: these samples are meant to check the training quality, so by default
#         the train image size is used!!! 
predictsample_image_input_subdir = ${train:image_pixel_width}x${train:image_pixel_height}
predictsample_image_input_dir = ${base_image_dir}/${predict:image_layer}_testsample/${predictsample_image_input_subdir}
predictsample_image_output_basedir = ${predictsample_image_input_dir}

[files]
# File path that will be used to save/load the keras model definition
model_json_filepath = ${dirs:model_dir}/${model:architecture}.json
image_layers_config_filepath = ${dirs:projects_dir}/imagelayers.ini

# File path of file that if it exists cancels the current processing
cancel_filepath = ${dirs:projects_dir}/cancel.txt

[email]
# Set enabled to True to enable sending mails
enabled = False
# Email address to send task status info from
from = sample@samplemail.be
# Email address to send task status info to
to = sample@samplemail.be
# Smtp server to use 
smtp_server = server.for.emails.be
# Username to use to login to smtp server (in some cases optional)
mail_server_username = 
# Password to use to login to smtp server (in some cases optional)
mail_server_password = 

[logging]
# Config to use for the logging. Mind: the location for file logging 
logconfig = {
        "version": 1,
        "disable_existing_loggers": true,
        "formatters": {
            "console": {
                "format": "%(asctime)s.%(msecs)03d|%(levelname)s|%(name)s|%(message)s", 
                "datefmt": "%H:%M:%S"
                },
            "file": {
                "format": "%(asctime)s|%(levelname)s|%(name)s|%(message)s", 
                "datefmt": null
            }
        },
        "handlers": {
            "console": {
                "level": "INFO",
                "class": "logging.StreamHandler",
                "formatter": "console",
                "stream": "ext://sys.stdout"
            },
            "file": {
                "level": "INFO",
                "class": "logging.handlers.RotatingFileHandler",
                "formatter": "file",
                "filename": "_log/{iso_datetime}.log",
                "maxBytes": 1024,
                "backupCount": 3
            }
        },
        "loggers": {
            "geofile_ops": {
                "level": "INFO",
                "handlers": ["console"],
                "propagate": false
            },
            "geofile_ops.geofile_ops": {
                "level": "DEBUG",
                "handlers": ["console"],
                "propagate": false
            }        
        },
        "root": {
            "level": "INFO",
            "handlers": ["console", "file"]
        }
    }